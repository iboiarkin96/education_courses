{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<center><span style=\"color:gray; font-weight: bold; font-size:18pt\">Непараметрические методы</span></center>\n",
    "<br/><br/>\n",
    "<center><span style=\"color: violet; font-weight: bold; font-size:14pt\">U-критерий Манна-Уитни</span></center>\n",
    "\n",
    "<span style=\"color: gray\">Непараметрический аналог <i>t</i>-критерия Стьюдента. Используется для оценки различий между двумя независимыми выборками, в которых признак измерен в метрической или ранговой шкале.</span>\n",
    "\n",
    "[Видео на Stepik](https://stepik.org/lesson/26822/step/3?unit=8557)\n",
    "\n",
    "Самым популярным непараметрическим критерием для сравнения двух групп является U-критерий Манна — Уитни. Логика данного критерия заключается в том, что вместо сравнения средних значений в двух выборках критерий сравнивает сумму рангов (не медианы, как многие думают). Мы сначала упорядочиваем все данные, затем рассчитываем сумму рангов в каждой из групп.\n",
    "\n",
    "Затем для каждой из выборок рассчитывается показатель:\n",
    "\n",
    "![U_1](pictures/U_1.jpg)\n",
    "\n",
    "Наименьшее из полученных значений и выступает в качестве статистики теста. Легко показать, что при условии верности нулевой гипотезы распределение этой статистики подчиняется нормальному распределению, где \n",
    "\n",
    "![U_2](pictures/U_2.jpg)\n",
    "\n",
    "что и позволяет нам рассчитать вероятность получить наблюдаемые или еще более выраженные различия суммы рангов.\n",
    "\n",
    "**Разумно применять вместо t - теста:**\n",
    "\n",
    "1. Распределения хотя бы в одной из выборок значительно отличается от нормального. \n",
    "2. Есть заметные выбросы в данных. \n",
    "3. В некоторых задачах мощность теста даже выше, чем t критерия (например, когда обеих выборках наблюдается заметная асимметрия в одинаковом направлении). \n",
    "\n",
    "**Неразумно применять:**\n",
    "\n",
    "1. Выборки разного размера, с различным направлением асимметрии.  \n",
    "\n",
    "<br/><br/>\n",
    "<center><span style=\"color: violet; font-weight: bold; font-size:14pt\">Критерий Краскела-Уоллиса</span></center>\n",
    "\n",
    "Если при сравнении трёх и более групп нарушаются требования и к гомогенности дисперсий и к нормальности распределений, лучше применять непараметрический аналог дисперсионного анализа - критерий Краскела-Уоллиса.\n",
    "\n",
    "![KruskalWallis_1](pictures/KruskalWallis_1.jpg)\n",
    "\n",
    "![KruskalWallis_2](pictures/KruskalWallis_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<center><span style=\"color:gray; font-weight: bold; font-size:18pt\">Кластерный анализ и метод главных компонент</span></center><br/>\n",
    "\n",
    "Метод кластерного анализа относится к группе методов \"обучение без учителя\". Это значит, что никто не знает правильного ответа на интересующий нас вопрос и нет никакой обратной связи.\n",
    "\n",
    "**Постановка задачи:** Разбить данные на группы\n",
    "\n",
    "<span style=\"color: orange; font-weight: bold; font-size:12pt\">Кластерный анализ</span> - анализирует наблюдения (по строкам), и отвечает на вопросы, есть ли в данных подгруппы/кластеры испытуемых и сколько таких кластеров лучше выделить.\n",
    "\n",
    "<span style=\"color: orange; font-weight: bold; font-size:12pt\">Метод главных компонент</span> - анализирует наблюдения (по столбцам), и отвечает на вопрос можно ли сократить размерность данных, объединив некоторые из них в группы (интегративные переменные).\n",
    "\n",
    "<br/><br/>\n",
    "<center><span style=\"color: violet; font-weight: bold; font-size:14pt\">Кластерный анализ методом k-средних</span></center>\n",
    "<br/>\n",
    "\n",
    "**Алгоритм метода k-средних:**\n",
    "1. Сами решаем на сколько кластеров будем делить.\n",
    "2. Случайно выбираем начальные позиции центроидов кластера.\n",
    "3. Для каждого наблюдения определяем, к какому центроиду он ближе всего.\n",
    "4. Обновим позиции центроидов (среднее по каждой переменной для группы).\n",
    "5. Если принадлежности некоторых точек изменились, то пункт 4, иначе алгоритм сошелся.\n",
    "\n",
    "<span style=\"color: orange; font-weight: bold; font-size:12pt\">Центроиды</span> - геометрические центры предполагаемых кластеров с координатами \\[ср.значение переменной OX в границах кластера; ср.значение переменной OY в границах кластера\\]\n",
    "\n",
    "[Визуализация метода](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)\n",
    "\n",
    "В методе существует элемент случайности. При многократном повторении кластеризации на одних и тех же данных мы можем получать различные варианты кластерного решения. Чем менее явно представлена в наших данных кластерзация наблюдений, тем более существенными могут оказаться различия. \n",
    "\n",
    "Возможно метод сойдется не очень удачно: метод “увяз” в локальном минимуме. \n",
    "**Решения:** \n",
    "+ Начальные точки брать наиболее далеко друг от друга; \n",
    "+ Провести кластерный анализ много раз с разными начальными позициями. (Если каждый раз из разных случайных начальных положений центроидов, кластерный анализ приходит в одно и то же положение, скорее всего, это не случайно)\n",
    "\n",
    "<br/>\n",
    "<span style=\"color: green; font-weight: bold; font-size:12pt\">Оптимальное число кластеров</span>\n",
    "\n",
    "Для того, чтобы выяснить, какое число кластеров оптимально, можно многократно проводить кластерный анализ, каждый раз выделяя разное кол-во кластеров и каждый раз забисываем значение общей внутрикластерной суммы квадратов.\n",
    "\n",
    "Если добавление одного кластера в наши данные значительно понижает общую сумму квадратов, то в увелечении числа кластеров есть смысл. Когда последующее увеличение кластеров уже не оказывает такого сильного влияния, значит мы нашли оптимальное число кластеров.\n",
    "\n",
    "Если при увелечении числа кластеров плавное снижение общей внтуригрупповой суммы квадратов, то значит нет явной класторной структуры в данных.\n",
    "\n",
    "![klaster](pictures/klaster.jpg)\n",
    "\n",
    "<ins>Расчёт внутригрупповой суммы квадратов в Python:</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 5.]]\n",
      "146.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#import numpy as np\n",
    "\n",
    "# Запишем координаты точек в виде массива numpy\n",
    "X = np.array([[-3, 3], [1, 4], [2, 6], [3, 8], [5, 2], [6, 11], [7, 1]])\n",
    "\n",
    "# Обучим модель KMeans на нашем массиве с одним кластером\n",
    "kmeans = KMeans(n_clusters=1).fit(X)\n",
    "\n",
    "# Выведем координаты центроида данного кластера\n",
    "print(kmeans.cluster_centers_)\n",
    "\n",
    "# Выведем сумму квадратов расстояний точек от центроида = аттрибут модели kmeans\n",
    "print(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<center><span style=\"color: violet; font-weight: bold; font-size:14pt\">Иерархическая кластеризация</span></center>\n",
    "\n",
    "**Идея метода:**\n",
    "+ Рассчитывается расстояние от каждой точки до каждой точки \n",
    "+ Производится кластеризация методами иереархической кластеризации, например:\n",
    " + [метод одиночной связи (ближайшего соседа)](https://stepik.org/lesson/27110/step/2?unit=8682) - Постепенно объеденяет две самые близкие точки в кластер, заменея их центроидом. В первую очередь объединяет самые близкие точки.\n",
    " + метод дальнего соседа - в последнюю очередь объединяет самые близкие точки.\n",
    " \n",
    "![klaster2](pictures/klaster2.png)\n",
    "\n",
    "![klaster1](pictures/klaster1.png)\n",
    "\n",
    "**Методы иерархической кластеризации и k-средних можно комбинировать**. Сначала применить метод иерархической кластеризации, оценить полученное количество веток, и использовать это число для метода k-средних."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<center><span style=\"color: violet; font-weight: bold; font-size:14pt\">Метод главных компонент</span></center> \n",
    " \n",
    "![pca](pictures/pca.png)\n",
    "\n",
    "В случае сильной корреляции двух переменных, регрессионная прямая может стать осью главной компоненты (РС1). Тогда значения новой интегративной переменной - это проекция(расстояние) на ось РС1. \n",
    "\n",
    "Значения по оси РС1 - это значения корреляции м/у двумя переменными. Например, OX-рост OY-вес, тогда РС1-конституция тела.\n",
    "\n",
    "Чем сильнее корреляция м/у переменными, тем меньше информации потеряется. При этом \"знак\" отклонения не учитывается, т.е. зная только значение по оси РС1 мы не можем знать, в какую сторону от регр. прямой отклоняется предсказ.наблюдение.\n",
    "\n",
    "Информация о \"знаке\" предсказ. значения может хранится в оси РС2 - перпендикуляру оси РС1. Но с учётом информациии по оси РС2, мы увеличиваем процент объяснённой дисперсии всего на 5%, что позволяет нам не учитывать эту информацию.\n",
    "\n",
    "![pca2](pictures/pca2.png)\n",
    "\n",
    "Таким образом, корреляционная прямая, новая ось РС1, становится одной новой переменной (интегративной переменной) вместо двух переменных по осям OX и OY. Это позволяет нам снизить размерность данных.\n",
    "\n",
    "![biplot](pictures/biplot.png)\n",
    "\n",
    "Если на графике biplot угол между переменными равен 90 градусов, значит коэффициент корреляции межу ними равняется нулю.\n",
    "\n",
    "[Пример анализа главных компонент, когда переменных больше двух](https://stepik.org/lesson/27111/step/6?unit=8681)\n",
    "\n",
    "<br/><br/>\n",
    "<center><span style=\"color: violet; font-weight: bold; font-size:14pt\">Факторный анализ</span></center>\n",
    "\n",
    "![factor](pictures/factor.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
